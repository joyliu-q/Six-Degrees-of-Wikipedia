{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import time \n",
    "import sys\n",
    "import numpy as np\n",
    "import argparse\n",
    "import string\n",
    "import concurrent\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, Future, as_completed, wait\n",
    "\n",
    "# Webscraping Imports\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import URLError, HTTPError\n",
    "import grequests\n",
    "\n",
    "# Viualization Imports\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grequest Exception Handler\n",
    "def exception_handler(request, exception):\n",
    "    print(request.text)\n",
    "    print(\"Request failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree Visualization\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Changable Variables\n",
    "USE_THREADPOOL = False\n",
    "MAKE_GRAPH = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dismissed links are links shared by all sites and do not factor into 6 degrees of separation\n",
    "dismissed_links = [\"Talk\", \"Categories\", \"Contributions\", \"Article\", \"Read\", \"Main page\", \"Contents\", \"Current events\", \"Random article\", \"About Wikipedia\", \"Help\", \"Community portal\", \"Recent changes\", \"Upload file\", \"What links here\", \"Related changes\", \"Upload file\", \"Special pages\", \"About Wikipedia\", \"Disclaimers\", \"Articles with short description\", \"Short description matches Wikidata\", \"Wikipedia indefinitely semi-protected biographies of living people\", \"Use mdy dates from October 2016\", \"Articles with hCards\", \"BLP articles lacking sources from October 2017\", \"All BLP articles lacking sources\", \"Commons category link from Wikidata\", \"Articles with IBDb links\", \"Internet Off-Broadway Database person ID same as Wikidata\", \"Short description is different from Wikidata\", \"PMID\", \"ISBN\", \"doi\"] \n",
    "degree = 0\n",
    "path = []\n",
    "path_found = False\n",
    "current_generation = []\n",
    "child_generation = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure args.fromtitle and args.totitle\n",
    "fromtitle = \"KEVIN bacon\"\n",
    "fromtitle = string.capwords(fromtitle.lower()).replace(\" \", \"_\")\n",
    "totitle = \"neo-nOIR\"\n",
    "totitle = string.capwords(totitle.lower()).replace(\" \", \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BS4 optimization\n",
    "only_a_tags = SoupStrainer(\"a\", href=lambda href: href and href.startswith('/wiki/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Node\n",
    "class Node:\n",
    "    def __init__(self, title, url):\n",
    "        self.title = title\n",
    "        self.url = url\n",
    "        self.parent = None\n",
    "        self.children = []\n",
    "        self.searched = False\n",
    "\n",
    "    def get_url(self):\n",
    "        return self.url\n",
    "\n",
    "# find_children - A function that returns all relevant referrals by Wikipedia\n",
    "def find_children(resp):\n",
    "    global USE_THREADPOOL\n",
    "    children = []\n",
    "    for r in resp:\n",
    "        soup = BeautifulSoup(r.text, \"lxml\", parse_only = only_a_tags)\n",
    "        soup = soup.find_all(\"a\")\n",
    "        for entry in soup:\n",
    "            if str(entry.get('title')) != \"[]\":\n",
    "                if \"/wiki/Help:\" in entry.get('href') or entry.get('title') == None or \"Wikipedia articles with\" in entry.get('title') or \"[<\" in entry.get('title') or \"<\" in str(entry.get('href')):\n",
    "                    continue\n",
    "                else:\n",
    "                    if entry.get('title') in dismissed_links:\n",
    "                        continue\n",
    "                    # If relevant, add to entries\n",
    "                    child_node = Node(entry.get('title'),\"https://en.wikipedia.org\" + entry[\"href\"])\n",
    "                    # Visualization\n",
    "                    children.append(child_node)\n",
    "    return children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attempt_match_children(current_node, to_node):\n",
    "    global child_generation\n",
    "    global path \n",
    "    global path_found\n",
    "    global USE_THREADPOOL\n",
    "\n",
    "    current_node.searched = True\n",
    "    # Check children for any matches w/ to_node \n",
    "    for child_node in current_node.children:\n",
    "        # Check if child_node was already searched\n",
    "        if child_node.searched == False:\n",
    "            # Add child_node to new generation\n",
    "            child_generation.append(child_node)\n",
    "            # If child_node is the to_node, search is over\n",
    "            if child_node.url == to_node.url:\n",
    "                path.append(current_node)\n",
    "                path.append(child_node)\n",
    "                path_found = True\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine amount of degrees and commonalities, with origin and end being urls\n",
    "def determine_path(from_node, to_node):\n",
    "    global path_found\n",
    "    global current_generation\n",
    "    global path\n",
    "    global degree\n",
    "    global child_generation \n",
    "    global USE_THREADPOOL\n",
    "\n",
    "    # 0th degree\n",
    "    if from_node.url == to_node.url:\n",
    "        path_found == True\n",
    "        return path\n",
    "\n",
    "    # 1st degree\n",
    "    degree += 1\n",
    "    resp = urlopen(from_node.url)\n",
    "    soup = BeautifulSoup(resp, 'html.parser', parse_only = only_a_tags)\n",
    "    for entry in soup:\n",
    "            if str(entry.contents) != \"[]\":\n",
    "                if \"/wiki/Help:\" in entry.contents[0] or \"Wikipedia articles with\" in entry.contents[0] or \"[<\" in entry.contents[0] or \"<\" in str(entry.contents[0]) or entry.contents[0] == None:\n",
    "                    continue\n",
    "                else:\n",
    "                    if entry.contents[0] in dismissed_links:\n",
    "                        continue\n",
    "                    # If relevant, add to entries\n",
    "                    child_node = Node(entry.contents[0],\"https://en.wikipedia.org\" + entry[\"href\"])\n",
    "                    # Visualization\n",
    "                    from_node.children.append(child_node)\n",
    "    attempt_match_children(from_node, to_node)\n",
    "\n",
    "    # 2+ degree: if none of the children match, continue to search through loop\n",
    "    if path_found == False:\n",
    "        current_node = from_node\n",
    "        current_generation = current_node.children\n",
    "        path.append(current_node)\n",
    "        \n",
    "        while path_found == False: \n",
    "            print(\"degree raised heh\")\n",
    "            degree += 1\n",
    "            child_generation = []\n",
    "            '''# Special Threadpool to find children: attempt to stop bottleneck\n",
    "            if USE_THREADPOOL == True:\n",
    "                with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "                    [executor.map(sibling_node.find_children()) for sibling_node in current_generation]\n",
    "            '''\n",
    "            \n",
    "            # Use G-requests to speed up url opening for response\n",
    "            rs = [grequests.get(sibling_node.url, params = sibling_node.url) for sibling_node in current_generation]\n",
    "            resp = grequests.map(rs, exception_handler=exception_handler)\n",
    "            current_generation = find_children(resp)\n",
    "            \n",
    "            # Keep Looping through each sibling_node and check sibling's children\n",
    "            for sibling_node in current_generation:\n",
    "                if sibling_node.searched == False:\n",
    "                    attempt_match_children(sibling_node, to_node)\n",
    "                    # If found match in current degree\n",
    "                    if path_found == True:\n",
    "                        return sibling_node\n",
    "            # If none of the siblings in the level matched, move to higher degree\n",
    "            if path_found == False:\n",
    "                current_generation = child_generation\n",
    "                child_generation = []\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global current_generation\n",
    "    start = time.time()\n",
    "\n",
    "    #root = Node(\"Kevin Bacon\", \"https://en.wikipedia.org/wiki/Kevin_Bacon\")\n",
    "    #target = Node(\"Neo-noir\", \"https://en.wikipedia.org/wiki/Hollywood\")\n",
    "\n",
    "    # Find/Check Valid Root and Target URL\n",
    "    try: \n",
    "        urlopen(\"https://en.wikipedia.org/wiki/\" + fromtitle)\n",
    "        print(\"https://en.wikipedia.org/wiki/\" + fromtitle)\n",
    "        print(\"https://en.wikipedia.org/wiki/\" + totitle)\n",
    "        urlopen(\"https://en.wikipedia.org/wiki/\" + totitle)\n",
    "    except HTTPError:\n",
    "        print(\"HTTPError: Invalid Name\")\n",
    "        sys.exit(1)\n",
    "    except URLError:\n",
    "        print(\"URLError: Invalid Name\")\n",
    "        sys.exit(1)\n",
    "    except ValueError:\n",
    "        print(\"ValueError: Invalid Name\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    root = Node(fromtitle.title(), \"https://en.wikipedia.org/wiki/\" + fromtitle)\n",
    "    target = Node(totitle.title(), \"https://en.wikipedia.org/wiki/\" + totitle)\n",
    "    \n",
    "    current_generation.append(root)\n",
    "    determine_path(root, target)\n",
    "    \n",
    "    # Present Data\n",
    "    path_names = []\n",
    "    for node in path:\n",
    "        path_names.append(node.title)\n",
    "    print(time.time() - start)\n",
    "    print(\"Path: \" + str(path_names))\n",
    "    print(\"Degree: \" + str(degree))\n",
    "\n",
    "    # Visualization of Path\n",
    "    if MAKE_GRAPH:\n",
    "        for i, node in enumerate(path):\n",
    "            G.add_node(node.title)\n",
    "            if i > 0:\n",
    "                G.add_edge(path[i-1].title, node.title)\n",
    "            print(node.url)\n",
    "        nx.draw(G, with_labels=True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/Kevin_Bacon\n",
      "https://en.wikipedia.org/wiki/Neo-noir\n",
      "degree_raised heh\n",
      "Request failed\n",
      "Request failed\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-c7bc734e5e35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-5710c86dd3e4>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mcurrent_generation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mdetermine_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Present Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-6bd386d92216>\u001b[0m in \u001b[0;36mdetermine_path\u001b[0;34m(from_node, to_node)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msibling_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msibling_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msibling_node\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcurrent_generation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexception_handler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexception_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mcurrent_generation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_children\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;31m# Keep Looping through each sibling_node and check sibling's children\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-f89ab743fbd4>\u001b[0m in \u001b[0;36mfind_children\u001b[0;34m(resp)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mchildren\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lxml\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_only\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monly_a_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
